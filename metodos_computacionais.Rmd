---
title: Comparação do estimador de máxima verossimilhança e estimador de máxima verossimilhança corrigido por bootstrap na Distribuição Gompertz

# use letras para afiliacao
author:
  - name: Ivonaldo Silvestre
    affiliation: a,b
  - name: Fabrício Emiliano
    affiliation: a,b
  - name: Alessandro Pereira
    affiliation: a,b
  - name: Leonardo Barros
    affiliation: a,b  
address:
  - code: a
    address: Departamento de Estatística - UFRN
  - code: b
    address: Aluno

# area do conhecimento (pelo menos uma)
subject:
  - "Métodos Computacionais em Estatística"

# palavras-chave
keywords:
  - Distribuição Gompertz
  - máxima verossimilhança
  - bootstrap

# resumo - note a barra vertical e os dois espaços no início do parágrafo
abstract: |
  O objetivo desse trabalho é avaliar, em termos de viés e erro quadrático médio, o estimador de máxima verossimilhança e o estimador de máxima verossimilhança corrigido por bootstrap, dos parâmetros a e b da Distribuição Gompertz. 

# arquivo com a bibliografia do relatorio
bibliography: modeloLEA.bib

header-includes:
   - \usepackage{bm}
   - \usepackage{float}
   - \usepackage{multirow}
   
output: 
  bookdown::pdf_book:
    base_format: modeloLEA::modeloLEA
---


```{r definicoes, include=FALSE, message=FALSE}
# pacotes a serem carregados para a analise
require(flexsurv)
library(xtable)
# definicoes globais 
# tamanho das figuras: largura=5, altura=4 (polegadas)
knitr::opts_chunk$set(fig.width=5, fig.height=4, 
# exibir o codigo do R nos chunks (mudar para FALSE na versao final)
                      echo=F,
# mandar o latex manter a figura na posicao correta
                      fig.pos='H')
```



# Introdução

Benjamin Gompertz foi um matemático judeu, que comprovou que a taxa de mortalidade cresce geometricamente e em 1825, no estudo __*On the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies*__, ele apresentou uma lei que descrevia o crescimento geométrico da taxa de mortalidade e "é um dos modelos matemáticos clássicos que representam a função de sobrevivência com base nas leis da mortalidade. Essa distribuição desempenha um papel importante na modelagem da mortalidade humana e no ajuste das tabelas atuariais." (A. El-Gohary et al., 2013, p.2, tradução nossa) \cite{el2013generalized}. Além disso, a distribuição Gompertz pode ser aplicada em diversas áreas do conhecimento, como "Gottwald et al. (1998) \cite{niblett2000progress} aplicaram o modelo de Gompertz para avaliar a ocorrência do vírus causador da "tristeza dos citros" na Costa Rica e República Dominicana. Marin et al. (1998) \cite{marin1998ecological} usaram essa função para modelar o crescimento dos fungos Aspergillus e Penicillium em milho." (Guimarães, 2002, p.12) \cite{guimaraes2002funccao} . 

# Metodologia

A estimação dos parâmetros é um dos pontos fundamentais para o ajuste de qual quer modelo e no presente trabalho serão apresentados dois métodos de estimação dos parâmetros, o método do estimador de máxima verossimilhança e o estimador de máxima verossimilhança utilizando Monte Carlo e corrigido por bootstrap. É importante ressaltar que as amostras da distribuição $Gompertz(a,b)$ serão geradas considerando como primeiro par $a=2$ e $b=3$ e como segundo $a=0.5$ e $b=0.5$. Assim, os estimadores serão avaliados para diferentes números de amostras, neste trabalho serão utilizados $n = 25,\ 50,\ 100$ e $200$.) Utilizando os chutes iniciais para a e b, podemos obter as estimativas pontuais para os parâmetros pelo método de máxima verossimilhança. O estudo foi desenvolvido utilizando o software R \citep{R2020}.

## Método de Máxima Verossimilhança

Seja $X$ uma variável aleatória que segue a distribuição $Gompertz(a,b)$, a sua função de densidade de probabilidade é dada por

$$ f(x;a,b) = be^{ax}exp[-b/a(e^{ax}-1)],\ x>0, a>0 \ e \ b>0 $$
em que $a$ representa o parâmetro de forma e $b$ representa o parâmetro de escala. Sua distribuição acumulada é dada por

$$ F(x;a,b) = 1 - \exp\bigg[\frac{-b}{a(e^{ax}-1)}\bigg],\ x>0.$$
Ao selecionar uma amostra aleatória de tamanho $n$, podemos encontrar a função de verossimilhança que é dada pela forma

$$L(a,b;\bm{x}) = b^n \exp\left(a\sum_{i=1}^nx_i\right)  \exp\left[-\left(\frac{b}{a}\right)\left(\sum_{i=1}^ne^{ax_i}-n\right)\right],$$
logo, a função log-verossimilhança é da forma

$$ l(a,b;\bm{x}) = n\ log (b) + a\sum_{i=1}^nx_i - \frac{b}{a}\left(\sum_{i=1}^ne^{ax_i}-n\right).$$
O estimador de máxima verossimilhança é o valor de $\theta$ que maximiza a função de verossimilhança $L(a,b;\bm{x})$. Em alguns casos, é mais fácil achar o $\theta$ que maximiza a função log-verossimilhança $l(a,b;\bm{x})$.

O vetor escore $\bm{U}(a,b;\bm{x}) = (U_a(a,b;\bm{x}),\ U(a,b;\bm{x}))^\top$ são dadas respectivamente por

\begin{align*}
U_a(a,b;\bm{x}) &= \sum_{i=1}^nx_i + \frac{b}{a^2}\left(\sum_{i=1}^n \left[e^{ax_i}(1-ax_i)\right]-n \right)
\\
U_b(a,b;\bm{x}) &= \frac{n}{b}-\frac{1}{a}\left(\sum_{i=1}^ne^{ax_i}-n\right).
\end{align*}
A solução dada por $\hat a$ e $\hat b$ quando $\bm{U}(a,b;\bm{x}) = 0$ não pode ser resolvida analiticamente, sendo necessários métodos numéricos para a resolução simultânea do vetor e devido a isso, todas as estimações no decorrer do trabalho serão feitas utilizando o método de Newton-Raphson para encontrar as estimativas de máxima verossimilhança. A matriz de informação de Fisher com os dados observados é uma matriz simétrica $2\times 2,$ da forma
$$I_0(\hat a,\hat b) = \left[
\begin{array}{cc}
A & B  \\
B & C
\end{array}
\right],$$

sendo

\begin{align*}
A & = \frac{\hat b}{\hat a^3} \left\{2\sum_{i=1}^n \left[e^{\hat a x_i}(1-\hat a x_i)-1\right] -\hat a \sum_{i=1}^n \left[e^{\hat a x_i}x_i[(1-\hat a x_i)^2-1]\right]  \right\}\\
B & = - \frac{1}{\hat a^2} \sum_{i=1}^n \left[e^{\hat a x_i} (1-\hat a x_i)-1 \right]\\
C & = \frac{n}{\hat b^2}.
\end{align*}

Logo, a inversa da matriz de informação de Fisher é 

$$I_0^{-1}=\frac{1}{AC-B^2}\left[
\begin{array}{cc}
C & -B  \\
-B & A
\end{array}
\right].$$

Assintoticamente, o vetor $(\hat a, \hat b)^\top$ segue uma distribuição normal de parâmetros

$$\left(\begin{array}{c}
\hat a \\
\hat b
\end{array}\right) \sim N\left( \left(\begin{array}{c}
a \\
b
\end{array}\right), I_0^{-1} \right).$$


Para construir o intervalo de confiança com $100(1-\alpha)\%$ para os parâmetros da distribuição considerou-se os quantis $\frac{\alpha}{2}$ e $1 - \left(\frac{\alpha}{2}\right)$ do conjunto das $i$ estimativas de máxima verossimilhança que foram gerados durante o processo de Monte Carlo.


## Método *Bootstrap*

O método bootstrap \citep{efron1992bootstrap} foi um método desenvolvido por Bradley Efron e seu artigo foi apresentado no _The Anals of Statistics_ em 1979. O bootstrap é um método de reamostragem que consiste em retirar subamostras a partir de uma amostra já coletada. Existem dois paradigmas para o bootstrap, o método de bootstrap não paramétrico e o bootstrap paramétrico. No presente trabalho aplicamos o método de bootstrap paramétrico que consiste em gerar $R=5000$ amostras da distribuição $Gompertz(a,b)$, para cada amostra $i$, $i=1,\dots,R$ calculam-se os estimadores de máxima verossimilhança para $a$ e $b$ que são $\hat a_{m, \ i}$,$\hat{b}_{m, \ i}$ e geram-se $B=500$ amostras da distribuição $Gompertz(\hat{a}_m,\hat{b}_m)$, e para cada amostra $j = 1,\dots, B$, obtém-se as estimativas de máxima verossimilhança por bootstrap de $a$ e $b$ que são chamados de $\hat{a}_{boot, \ j}$ e $\hat{b}_{boot,\ j}$. 

Após esse processo calcula-se a média das estimativas de bootstrap para originar a estimativa pontual:

\begin{align*}
\hat{a}_{boot,i} &= \frac{\hat{a}_{boot, \ ij}}{B}\\ \hat{b}_{boot,i} &= \frac{\hat{b}_{boot,\ ij}}{B}
\end{align*}

As estimativas corrigidas para $a$ e $b$ por esse método é obtido multiplicando duas vezes a $i$-ésima estimativa de máxima verossimilhança de $a$ e $b$ menos a estimativa de bootstrap:

\begin{align*}
\hat{a}_{corrigido,i} &= 2\hat{a}_i -\hat{a}_{boot,i} \\ \hat{b}_{corrigido,i} &= 2\hat{b}_i - \hat{b}_{boot,i}
\end{align*}

A estimativa pontual dos parâmetros da distribuição utilizando esse método é obtido pela média das $i$ estimativas, $i=1,\dots, R$

\begin{align*}
\hat{a}_{corrigido} &= \frac{\hat{a}_{corrigido,i}}{R} \\ \hat{b}_{corrigido} &= \frac{\hat{b}_{corrigido,i}}{R}
\end{align*}

O viés é obtido diminuindo a estimativa pontual e o parâmetro usado para gerar as amostras, portanto:

\begin{align*}
B(\hat a_{corrigido}) &= \hat a_{corrigido} - a \\
B(\hat b_{corrigido}) &= \hat b_{corrigido} - b
\end{align*}

A variância é obtida aplicando-se a fórmula do $S^2$ no conjunto dos valores de $\hat{a}_{corrigido,i}$ e $\hat{b}_{corrigido,i}$. Percebe-se que nos somatórios abaixo utilizou-se a estimativa pontual dos parâmetros pois tal estimativa consiste da média das $i$ observações geradas, para $i=1,\dots,R$. Portanto,


\begin{align*}
\widehat{var}(\hat{a}_{corrigido}) &= \frac{1}{n-1}\sum_{i=1}^{R} \hat{a}_{corrigido,i} - \hat a_{corrigido} \\ 
\widehat{var}(\hat{b}_{corrigido}) &= \frac{1}{n-1}\sum_{i=1}^{R} \hat{b}_{corrigido,i} - \hat b_{corrigido}
\end{align*}


O erro quadrático médio é obtido como sendo a soma da variância mais o viés ao quadrado.

\begin{align*}
EQM(\hat a_{corrigido}) &= \widehat{var}(\hat{a}_{corrigido}) + B(\hat a_{corrigido})^2 \\ 
EQM(\hat b_{corrigido}) &= \widehat{var}(\hat{b}_{corrigido}) + B(\hat b_{corrigido})^2
\end{align*}

e, por fim, para construir o intervalo de confiança com $100(1-\alpha)\%$ para os parâmetros da distribuição considerou-se os quantis $\frac{\alpha}{2}$ e $1 - \left(\frac{\alpha}{2}\right)$ do conjunto das $i$ estimativas de máxima verossimilhança que foram gerados pelo processo de bootstrap e Monte Carlo, para $i=1,\dots,R$.

# Resultados

  Primeiramente, vamos usar o Método da Transformação inversa para gerar números pseudos-aleatórios que seguem a distribuição Gompertz. A Figura \ref{fig:comp} comparando os n números pseudos-aleatórios gerados usando o métodos da transformada inversa e n números gerados obtidos a partir da função de densidade de probabilidade da distribuição Gompertz a partir do pacote *flexsurv*.
  

```{r geração}

rgomp=function(n,a,b){
  u=runif(n)
  log(1 -a/b*log(1-u))/a #Inversa da função de distribuição acumulada
}

pfun1 <- function(x,a,b){
  f <- (b*(exp(1)^(a*x)))*exp((-b/a)*((exp(1)^(a*x))-1)) #Densidade
  return(f)
}

n=5000
a=1
b=1
dados=rgomp(n,a,b)
dados2=rgompertz(n,a,b)
x <- seq(0, max(dados), 0.01)

```
```{r comp, figures-side, fig.show="hold",out.width="49%",fig.cap="\\label{fig:comp}Comparação entre os números pseudo-aleatórios gerados"}  
#Histograma dos dados gerados.
hist(dados2,freq = FALSE,main="",xlab="x",ylab="Density-fun",lwd=2)
lines(x,pfun1(x,a,b),lty=1,lwd=2)
box()

#Histograma dos dados obtidos pela função densidade.
hist(dados,freq = FALSE,main="",xlab="x",ylab="Density-uni",lwd=2)
lines(x,pfun1(x,a,b),lty=1,lwd=2)
box()
```

```{r chute}

#Log da função de verossimilhança
logdgomp=function(x,dados){
  a=x[1]
  b=x[2]
  -sum(log(b)+a*dados-b/a*(exp(a*dados)-1))
}


achute=1 #Chute inicial para o parâmetro a
bchute=1 #Chute inicial para o parâmetro b
chute=c(achute,bchute)
```
\noindent
A Tabela \ref{tab:a2b3} mostra os resultados para $a=2$ e para $b=3$ das estimativas por Monte Carlo e corrigidas via *Bootstrap*. É visível que em ambos os casos, o viés do corrigido é consideralvemente menor que o do Monte Carlo. No caso do $a,$ todos os erros quadráticos foram menores no caso corrigido, entretanto, no caso do $b,$ todos os erros quadráticos dos estimadores corrigidos foram levemente superiores. Com $n=200,$ a estimativa corrigida de $a$ já acertou até a terceira casa decimal em caso de arredondamento, já a estimativa corrigida do $b$ com $n=200$ acertou até a segunda casa decimal ao arredondar. Nenhuma estimativa de Monte Carlo acertou pontualmente em pelo menos duas casas decimais. A Tabela \ref{tab:ica2b3} mostra que todos os intervalos com 95\% de confiança para cada estimador contiveram o verdadeiro valor do parâmetro.

\begin{table}[H]
\resizebox{\linewidth}{!}{ 
\centering
\begin{tabular}{ll|rrrr|rrrr}
 \hline
& & \multicolumn{4}{c|}{Estimativas para $a=2$} & \multicolumn{4}{c}{Estimativas de $b=3$}\\
  \hline
n & Estimador & Média & Viés & Variância & EQM & Média & Viés & Variância & EQM \\ 
  \hline
    25 & Monte Carlo & 2,5415 & 0,5415 & 1,9746 & 2,2678 & 2,9216 & -0,0784 & 1,0622 & 1,0683 \\ 
    25 & Corrigido & 1,9412 & -0,0588 & 1,9935 & 1,9969 & 3,0113 & 0,0113 & 1,1300 & 1,1301 \\ 
    50 & Monte Carlo & 2,2691 & 0,2691 & 0,8237 & 0,8962 & 2,9540 & -0,0460 & 0,5087 & 0,5108 \\ 
    50 & Corrigido & 1,9791 & -0,0209 & 0,8386 & 0,8391 & 3,0074 & 0,0074 & 0,5389 & 0,5389 \\ 
    100 & Monte Carlo & 2,1491 & 0,1491 & 0,3862 & 0,4084 & 2,9623 & -0,0377 & 0,2505 & 0,2519 \\ 
  100 & Corrigido & 2,0071 & 0,0071 & 0,3913 & 0,3913 & 2,9902 & -0,0098 & 0,2589 & 0,2590 \\ 
  200 & Monte Carlo & 2,0711 & 0,0711 & 0,1794 & 0,1845 & 2,9824 & -0,0176 & 0,1229 & 0,1232 \\ 
   200 & Corrigido & 2,0004 & 0,0004 & 0,1808 & 0,1808 & 2,9969 & -0,0031 & 0,1252 & 0,1252 \\ 
   \hline
\end{tabular}
}
\caption{Comparação dos Estimadores pelos métodos de Monte Carlo e corrigido por Bootstrap para $a=2$ e $b=3$.\label{tab:a2b3}}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ll|rr|rr}
  \hline
& & \multicolumn{2}{c|}{Estimativas para $a=2$} & \multicolumn{2}{c}{Estimativas de $b=3$}\\
  \hline
  n & Estimador & Limite Inferior & Limite Superior & Limite Inferior & Limite Superior \\ 
  \hline
    25 & Monte Carlo &0,2651 & 5,7284 & 1,3015 & 5,2711 \\ 
    25 & Corrigido   &0,0000 & 5,0533 & 1,3261 & 5,4352 \\ 
    50 & Monte Carlo &0,7178 & 4,2411 & 1,7390 & 4,5267 \\ 
    50 & Corrigido   &0,4002 & 3,9604 & 1,7557 & 4,6239 \\ 
   100 & Monte Carlo &1,0380 & 3,4404 & 2,0915 & 4,0425 \\ 
   100 & Corrigido   &0,8706 & 3,3091 & 2,1037 & 4,0872 \\ 
   200 & Monte Carlo &1,2957 & 2,9446 & 2,3342 & 3,7138 \\ 
   200 & Corrigido   &1,2204 & 2,8711 & 2,3464 & 3,7296 \\ 
   \hline
\end{tabular}
\caption{Intervalos com 95$\%$ de confiança do estimadores pelos métodos de Monte Carlo e corrigido por Bootstrap para $a=2$ e $b=3$.\label{tab:ica2b3}}
\end{table}




\noindent
A Tabela \ref{tab:a5b5} mostra os resultados para $a=b=0,5.$ Essa combinação teve resultados similares aos da primeira, ou seja, ambos os parâmetros mantiveram com o viés menor nas estimativas corrigidas, entretanto o EQM da estimativa corrigida do $b$ ficou maior que o de Monte Carlo. A partir de $n=50,$ já é possível perceber um acerto pontual na estimativa em até duas casas decimais, chegando até a 4 com $n=200,$ para $a$ no estimador corrigido, o que não ocorreu em nenhum caso no Monte Carlo. No caso do $b,$ com $n=25$ já era possível ver um acerto com 2 casas decimais no estimador corrigido, entretanto, no Monte Carlo isso só veio a ocorrer com $n=200$. A Tabela \ref{tab:ica5b5} mostra os respectivos limites inferior e superior dos intervalos com 95\% de confiança para $a$ e $b$. Todos contiveram o real valor do parâmetro e no geral tiveram valores bem próximos comparando os corrigidos e não corrigidos em cada tamanho de amostra. 

\begin{table}[H]
\resizebox{\linewidth}{!}{ 
\centering
\begin{tabular}{ll|rrrr|rrrr}
 \hline
& & \multicolumn{4}{c|}{Estimativas para $a=0,5$} & \multicolumn{4}{c}{Estimativas de $b=0,5$}\\
  \hline
n & Estimador & Média & Viés & Variância & EQM & Média & Viés & Variância & EQM \\ 
    25 & Monte Carlo & 0,6011 & 0,1011 & 0,0766 & 0,0869 & 0,4887 & -0,0113 & 0,0322 & 0,0323 \\ 
    25 & Corrigido & 0,4872 & -0,0128 & 0,0774 & 0,0775 & 0,5036 & 0,0036 & 0,0349 & 0,0349 \\ 
   50 & Monte Carlo & 0,5504 & 0,0504 & 0,0326 & 0,0351 & 0,4932 & -0,0068 & 0,0154 & 0,0154 \\ 
    50 & Corrigido & 0,4961 & -0,0039 & 0,0329 & 0,0329 & 0,5013 & 0,0013 & 0,0162 & 0,0162 \\ 
   100 & Monte Carlo & 0,5278 & 0,0278 & 0,0154 & 0,0162 & 0,4942 & -0,0058 & 0,0076 & 0,0076 \\ 
   100 & Corrigido & 0,5013 & 0,0013 & 0,0155 & 0,0155 & 0,4984 & -0,0016 & 0,0078 & 0,0078 \\ 
   200 & Monte Carlo & 0,5132 & 0,0132 & 0,0072 & 0,0074 & 0,4973 & -0,0027 & 0,0037 & 0,0037 \\ 
   200 & Corrigido & 0,5000 & 0,0000 & 0,0072 & 0,0072 & 0,4995 & -0,0005 & 0,0038 & 0,0038 \\ 
   \hline
\end{tabular}
}
\caption{Comparação dos Estimadores pelos métodos de Monte Carlo e corrigido por Bootstrap para $a=0,5$ e $b=0,5$.\label{tab:a5b5}}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ll|rr|rr}
  \hline
& & \multicolumn{2}{c|}{Estimativas para $a=0,5$} & \multicolumn{2}{c}{Estimativas de $b=0,5$}\\
  \hline
  n & Estimador & Limite Inferior & Limite Superior & Limite Inferior & Limite Superior \\ 
  \hline
  25 & Monte Carlo & 0,1469 & 1,2189 & 0,2073 & 0,9030 \\ 
  25 & Corrigido    & 0,0191 & 1,0902 & 0,2116 & 0,9364 \\ 
  50 & Monte Carlo & 0,2396 & 0,9393 & 0,2828 & 0,7687 \\ 
  50 & Corrigido    & 0,1793 & 0,8840 & 0,2856 & 0,7815 \\ 
 100 & Monte Carlo & 0,3016 & 0,7847 & 0,3436 & 0,6819 \\ 
 100 & Corrigido    & 0,2710 & 0,7584 & 0,3449 & 0,6895 \\ 
 200 & Monte Carlo & 0,3567 & 0,6868 & 0,3861 & 0,6239 \\ 
 200 & Corrigido    & 0,3431 & 0,6743 & 0,3871 & 0,6269 \\ 
   \hline
\end{tabular}
\caption{Intervalos com 95$\%$ de confiança do estimadores pelos métodos de Monte Carlo e corrigido por Bootstrap para $a=0,5$ e $b=0,5$.\label{tab:ica5b5}}
\end{table}


```{r tabelas}
tabela25=read.csv2("Tabela25.csv")
tabela50=read.csv2("Tabela50.csv")
tabela100=read.csv2("Tabela100.csv")
tabela200=read.csv2("Tabela200.csv")
a2b3=rbind(tabela25[1:4,],tabela50[1:4,],tabela100[1:4,],tabela200[1:4,])
a2b3=a2b3[order(a2b3$`Parâmetro`),]
#a2b3=a2b3[,c(1,5:8)]
a2b3=a2b3[,c(1,9,10)]
a2b3=cbind(a2b3[1:8,],a2b3[9:16,])
#a2b3=a2b3[,-6]
a2b3=a2b3[,-4]
a5b5=rbind(tabela25[-c(1:4),],tabela50[-c(1:4),],
           tabela100[-c(1:4),],tabela200[-c(1:4),])
a5b5=a5b5[order(a5b5$`Parâmetro`),]
#a5b5=a5b5[,c(1,5:8)]
a5b5=a5b5[,c(1,9,10)]
a5b5=cbind(a5b5[1:8,],a5b5[9:16,])
#a5b5=a5b5[,-6]
a5b5=a5b5[,-4]

#xtable(a2b3,digits=4)
#xtable(a5b5,digits=4)
```

  
```{r estimativas}
#Estimador de máxima verossimilhança 
estimativas=optim(par=chute,fn=logdgomp,dados=dados)$par
ahat=estimativas[1] #Estimador pontual para a
bhat=estimativas[2] #Estimador pontual para b
```

```{r IC}
C=n/bhat^2
B=-sum(exp(ahat*dados)*(1-ahat*dados)-1)/ahat^2
A=bhat/ahat^3*(2*(sum(exp(ahat*dados)*(1-ahat*dados))-1) -
           ahat*sum(exp(ahat*dados)*dados*((1-ahat*dados)^2-1))
         )
varahat=C/(A*C-B^2)
varbhat=A/(A*C-B^2)
covabhat=-B/(A*C-B^2)
#IC para o parâmetro a Máxima verossimilhança
LimInf_a_MV = quantile(ahat,0.025)
LimSup_a_MV = quantile(ahat,0.975)
# IC para o parâmetro b Máxima verossimilhança
LimInf_b_MV = quantile(bhat,0.025)
LimSup_b_MV = quantile(bhat,0.975)



```


```{r informacoes}
# usando n = 25, 50, 100 e 200
# R=5000
# B=500
# 2 configuracoes de parametros
```

# Conclusão
Para a distribuição Gompertz, as estimativas de Monte Carlo com correção de Bootstrap têm viés menor, entretanto não necessariamente uma variância e erro quadrático médio menores que as não corrigidas, por conseguinte, não necessariamente têm um intervalo de confiança menor.


# Códigos utilizados


```{r códigos, echo=T, eval=FALSE}
#Pacotes utilizados:
require(flexsurv)
library(doParallel)

#Função para geração de números aleatórios da densidade da distribuição gompertz(a,b)
rgomp=function(n,a,b){
  u=runif(n)
  log(1 -a/b*log(1-u))/a #Inversa da função de distribuição acumulada
}


# Função de densidade de probabilidade. (Essa a gente usa no histograma)
pfun1 <- function(x,a,b){
  f <- (b*(exp(1)^(a*x)))*exp((-b/a)*((exp(1)^(a*x))-1)) #Densidade
  return(f)
}

# Função log-verossimilhança da Gompertz:

logdgomp=function(x,dados){
  a=x[1]
  b=x[2]
  -sum(log(b)+a*dados-b/a*(exp(a*dados)-1))
}

#Parâmetros 
n = 200
R = 5000
B = 500
a = 2
b = 3
chute=c(a,b)
set.seed(1024122)
#Como estamos utilizando achute = a = 2 e bchute = b = 3 então eu coloquei que chute = c(a,b) só para facilitar nas alterações
#achute=2 #Chute inicial para o parâmetro a
#bchute=3 #Chute inicial para o parâmetro b

#Alocação de memória
ahat_MV<-rep(NA,R)
bhat_MV<-rep(NA,R)
ahat_boot<-matrix(NA,nrow=R,ncol=B)
bhat_boot<-matrix(NA,nrow=R,ncol=B)
ahat_MV_boot<-rep(NA,R)
bhat_MV_boot<-rep(NA,R)

#Alocação de clusteres para otimizar os laços do monte carlo e bootstrap
no_cores <- detectCores()-2
cl <- makeCluster(no_cores)  
registerDoParallel(cl)  


foreach(i=1:R)%do%{
  Z = rgomp(n,a,b)
  
  # Utilizando o método numérico "BFGS" para estimar os parâmtros a e b da distribuição gompertz pelo método da máxima verossimilhança
  estimados_MV <- optim(par=chute, method="BFGS", fn = logdgomp, dados=Z)
  ahat_MV[i] <- estimados_MV$par[1]
  bhat_MV[i]      <- estimados_MV$par[2]
  
  
  foreach(j=1:B)%do%{
    Z_boot = rgomp(n,ahat_MV[i],bhat_MV[i])
    estimados_boot <- optim(par=c(ahat_MV[i],bhat_MV[i]), method="BFGS", fn = logdgomp, dados=Z_boot)
    
    ahat_boot[i,j] <- estimados_boot$par[1]
    bhat_boot[i,j] <- estimados_boot$par[2]
  }
  
  ahat_MV_boot[i] <- mean(ahat_boot[i,],na.rm=T)
  bhat_MV_boot[i] <- mean(bhat_boot[i,],na.rm=T)
  
  print(paste0(round(((i)/5000)*100,1),"% concluído"))
}

#Fechando a alocação de clusteres
stopCluster(cl)

#Estimação pontual pelo método de monte carlo
estimate_a <- mean(ahat_MV)
estimate_b <- mean(bhat_MV)

estimate_a
estimate_b

#Cálculo do Viés e do Erro Quadrático Médio (EQM)
vies_ahat <- mean(ahat_MV) - a
vies_bhat <- mean(bhat_MV) - b

vies_ahat
vies_bhat 

var_ahat<-var(ahat_MV)
var_bhat<-var(bhat_MV)

EQM_a <- vies_ahat^2 + var_ahat
EQM_b <- vies_bhat^2 + var_bhat

EQM_a
EQM_b 

# IC 95%

LimInf_a_MV = quantile(ahat_MV,0.025)
LimSup_a_MV = quantile(ahat_MV,0.975)


LimInf_b_MV = quantile(bhat_MV,0.025)
LimSup_b_MV = quantile(bhat_MV,0.975)



#Cálculo do Viés e do Erro Quadrático Médio (EQM)


a_corrigido <- 2*ahat_MV - ahat_MV_boot
b_corrigido <- 2*bhat_MV - bhat_MV_boot

a_corrigido
b_corrigido

estimate_a_cor = mean(a_corrigido,na.rm=T)
estimate_b_cor = mean(b_corrigido,na.rm=T)

vies_a_cor <- estimate_a_cor - a
vies_b_cor <- estimate_b_cor - b

var_a_cor<-var(a_corrigido,na.rm=T)
var_b_cor<-var(b_corrigido,na.rm=T)

EQM_a_cor <- vies_a_cor^2 + var_a_cor
EQM_b_cor <- vies_b_cor^2 + var_b_cor


# IC 95%

LimInf_a_cor = quantile(a_corrigido,0.025,na.rm=T)
LimSup_a_cor = quantile(a_corrigido,0.975,na.rm=T) 

LimInf_b_cor = quantile(b_corrigido,0.025,na.rm=T)
LimSup_b_cor = quantile(b_corrigido,0.975,na.rm=T) 

#_________//_________//_________//_________//_________//_________//_________//____


# Criando a tabela que apresenta os resultados:

Método<-rep(c("Máxima Verossimilhança","Corrigido"),each=2)

# Monte carlo
Estimativa_mv<-as.matrix(c(estimate_a,estimate_b))

Vies_MV<-as.matrix(c(vies_ahat,vies_bhat))

Var_MV<-as.matrix(c(var_ahat,var_bhat))

EQM_MV<-as.matrix(c(EQM_a,EQM_b))

# Corrigido

Estimativa_cor <- as.matrix(c(estimate_a_cor,estimate_b_cor))

Vies_cor <- as.matrix(c(vies_a_cor,vies_b_cor))

Var_cor<-as.matrix(c(var_a_cor,var_b_cor))

EQM_Corrigido <- as.matrix(c(EQM_a_cor,EQM_b_cor))

# Criando a tabela
Parâmetro<-rep(c("a","b"),times=2)
Valor_do_parametro=rep(c(a,b),times=2)
Estimativa=rbind(Estimativa_mv,Estimativa_cor)
Vies<-rbind(Vies_MV,Vies_cor)
Var<-rbind(Var_MV,Var_cor)
EQM<-rbind(EQM_MV,EQM_Corrigido)
ICinf=rbind(LimInf_a_MV,LimInf_b_MV,LimInf_a_cor,LimInf_b_cor)
ICsup=rbind(LimSup_a_MV,LimSup_b_MV,LimSup_a_cor,LimSup_b_cor)
nrep=rep(n,4)
tabela<-data.frame(nrep,Método,Parâmetro,Valor_do_parametro,Estimativa,Vies,Var,EQM,ICinf,ICsup)


row.names(tabela)=1:4
names(tabela)[c(4,7,9,10)]<-c("Valor do parâmetro","Variância","Lim. Inf. 2.5%","Lim. Sup. 97.5%")


``` 
