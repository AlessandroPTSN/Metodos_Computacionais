---
title: Comparação do estimador de máxima verossimilhança e estimador de máxima verossimilhança corrigido por bootstrap na Distribuição Gompertz

# use letras para afiliacao
author:
  - name: Ivonaldo Silvestre
    affiliation: a,b
  - name: Fabrício Emiliano
    affiliation: a,b
  - name: Alessandro Pereira
    affiliation: a,b
  - name: Leonardo Barros
    affiliation: a,b  
address:
  - code: a
    address: Departamento de Estatística - UFRN
  - code: b
    address: Aluno

# area do conhecimento (pelo menos uma)
subject:
  - "Métodos Computacionais em Estatística"

# palavras-chave
keywords:
  - Distribuição Gompertz
  - máxima verossimilhança
  - bootstrap

# resumo - note a barra vertical e os dois espaços no início do parágrafo
abstract: |
  O objetivo desse trabalho é avaliar, em termos de viés e erro quadrático médio, o estimador de máxima verossimilhança e o estimador de máxima verossimilhança corrigido por bootstrap, dos parâmetros a e b da Distribuição Gompertz. 

# arquivo com a bibliografia do relatorio
bibliography: modeloLEA.bib

header-includes:
   - \usepackage{bm}
   - \usepackage{float}
   - \usepackage{multirow}
   
output: 
  bookdown::pdf_book:
    base_format: modeloLEA::modeloLEA
---


```{r definicoes, include=FALSE, message=FALSE}
# pacotes a serem carregados para a analise
require(flexsurv)
library(xtable)
# definicoes globais 
# tamanho das figuras: largura=5, altura=4 (polegadas)
knitr::opts_chunk$set(fig.width=5, fig.height=4, 
# exibir o codigo do R nos chunks (mudar para FALSE na versao final)
                      echo=F,
# mandar o latex manter a figura na posicao correta
                      fig.pos='H')
```



# Introdução

Benjamin Gompertz foi um matemático judeu, que comprovou que a taxa de mortalidade cresce geometricamente e em 1825, no estudo __*On the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies*__, ele apresentou uma lei que descrevia o crescimento geométrico da taxa de mortalidade e "é um dos modelos matemáticos clássicos que representam a função de sobrevivência com base nas leis da mortalidade. Essa distribuição desempenha um papel importante na modelagem da mortalidade humana e no ajuste das tabelas atuariais." (A. El-Gohary et al., 2013, p.2, tradução nossa). Além disso, a distribuição Gompertz pode ser aplicada em diversas areas de conhecimento, como "(Gottwald et al. (1998) aplicaram o modelo de Gompertz para avaliar a ocorrência do vírus causador da "tristeza dos citros" na Costa Rica e República Dominicana. Marin et al. (1998) usaram essa função para modelar o crescimento dos fungos Aspergillus e Penicillium em milho." (Guimarães, 2002, p.12). 

# Metodologia

A estimação dos parâmetros é um dos pontos fundamentais para o ajuste de um modelo. E neste trabalho vamos mostrar dois métodos diferentes de estimação dos parâmetros, o método do estimador de máxima verossimilhança e o estimador por monte carlo corrigido por bootstrap. É importante ressaltar que vamos estimar dois pares de valores diferentes dos parâmetros a e b, primeiramente vamos estimar $a=2$ e $b=3$, e também vamos estimar $a=0.5$ e $b=0.5$. Assim como iremos avaliar os estimadores para diferentes números de amostras, neste trabalho serão utilizados $n = 25,\ 50,\ 100$ e $200$.) Utilizando os chutes iniciais para a e b, podemos obter as estimativas pontuais para os parâmetros pelo método de máxima verossimilhança.

## Método de Máxima Verossimilhança

Seja $X$ uma variável aleatória que segue a distribuição Gompertz, a sua função de densidade de probabilidade é dada por

$$ f(x;a,b) = be^{ax}exp[-b/a(e^{ax}-1)],\ x>0, $$
em que $a > 0$ representa o parâmetro de forma e $b>0$ representa o parâmetro de escala. Sua distribuição acumulada é dada por

$$ F(x;a,b) = 1 - exp[-b/a(e^{ax}-1)],\ x>0.$$
Ao selecionar uma amostra aleatória de tamanho $n,$ podemos encontrar a função de verossimilhança é dada pela forma

$$L(a,b;\bm{x}) = b^nexp\left(a\sum_{i=1}^nx_i\right) exp\left[-\frac{b}{a}\left(\sum_{i=1}^ne^{ax_i}-n\right)\right],$$
logo, a função log-verossimilhança é da forma

$$ l(a,b;\bm{x}) = n\ log (b) + a\sum_{i=1}^nx_i - \frac{b}{a}\left(\sum_{i=1}^ne^{ax_i}-n\right).$$
O estimador de máxima verossimilhança é o valor $\theta$ que maximiza a função de verossimilhança $L(a,b;\bm{x})$. Em alguns casos, é mais fácil achar o $\theta$ que maximiza a função log-verossimilhança $l(a,b;\bm{x})$.

O vetor escore $\bm{U}(a,b;\bm{x}) = (U_a(a,b;\bm{x}),\ U(a,b;\bm{x}))^\top$ são dadas respectivamente por

\begin{align*}
U_a(a,b;\bm{x}) &= \sum_{i=1}^nx_i + \frac{b}{a^2}\left(\sum_{i=1}^n \left[e^{ax_i}(1-ax_i)\right]-n \right)
\\
U_b(a,b;\bm{x}) &= \frac{n}{b}-\frac{1}{a}\left(\sum_{i=1}^ne^{ax_i}-n\right).
\end{align*}
A solução dada por $\hat a$ e $\hat b$ quando $\bm{U}(a,b;\bm{x}) = 0$ não pode ser resolvida analiticamente, sendo necessários métodos computacionais para a resolução simultânea do vetor. A matriz de informação de Fisher com os dados observados é uma matriz simétrica $2\times 2,$ da forma
$$I_0(\hat a,\hat b) = \left[
\begin{array}{cc}
A & B  \\
B & C
\end{array}
\right],$$

sendo

\begin{align*}
A & = \frac{\hat b}{\hat a^3} \left\{2\sum_{i=1}^n \left[e^{\hat a x_i}(1-\hat a x_i)-1\right] -\hat a \sum_{i=1}^n \left[e^{\hat a x_i}x_i[(1-\hat a x_i)^2-1]\right]  \right\}\\
B & = - \frac{1}{\hat a^2} \sum_{i=1}^n \left[e^{\hat a x_i} (1-\hat a x_i)-1 \right]\\
C & = \frac{n}{\hat b^2}.
\end{align*}

Logo, a inversa da matriz de informação de Fisher é 

$$I_0^{-1}=\frac{1}{AC-B^2}\left[
\begin{array}{cc}
C & -B  \\
-B & A
\end{array}
\right].$$

Assintoticamente, o vetor $(\hat a, \hat b)^\top$ segue uma distribuição normal de parâmetros

$$\left(\begin{array}{c}
\hat a \\
\hat b
\end{array}\right) \sim N\left( \left(\begin{array}{c}
a \\
b
\end{array}\right), I_0^{-1} \right).$$

Portanto, para construir um intervalo com $100(1-\alpha)\%$ de confiança para $a$ e $b,$ usa-se as seguintes fórmulas

\begin{align*}
IC[a;100(1-\alpha)\%] &= \left[\hat a - Z_{\alpha/2}\sqrt{\frac{C}{AC-B^2}};\hat a + Z_{1-\alpha/2}\sqrt{\frac{C}{AC-B^2}} \right]\\
IC[b;100(1-\alpha)\%] &= \left[\hat b - Z_{\alpha/2}\sqrt{\frac{A}{AC-B^2}};\hat b + Z_{1-\alpha/2}\sqrt{\frac{A}{AC-B^2}} \right],
\end{align*}

em que $Z_q$ representa o quantil da distribuição normal padrão com probabilidade acumulada $q$.

# Resultados

  Primeiramente, vamos usar o Método da Transformação inversa para gerar números pseudos-aleatórios que seguem a distribuição Gompertz. A Figura \ref{fig:comp} comparando os n números pseudos-aleatórios gerados usando o métodos da transformada inversa e n números gerados obtidos a partir da função de densidade de probabilidade da distribuição Gompertz a partir do pacote *flexsurv*.
  

```{r geração}

rgomp=function(n,a,b){
  u=runif(n)
  log(1 -a/b*log(1-u))/a #Inversa da função de distribuição acumulada
}

pfun1 <- function(x,a,b){
  f <- (b*(exp(1)^(a*x)))*exp((-b/a)*((exp(1)^(a*x))-1)) #Densidade
  return(f)
}

n=5000
a=1
b=1
dados=rgomp(n,a,b)
dados2=rgompertz(n,a,b)
x <- seq(0, max(dados), 0.01)

```
```{r comp, figures-side, fig.show="hold",out.width="49%",fig.cap="\\label{fig:comp}Comparação entre os números pseudo-aleatórios gerados"}  
#Histograma dos dados gerados.
hist(dados2,freq = FALSE,main="",xlab="x",ylab="Density-fun",lwd=2)
lines(x,pfun1(x,a,b),lty=1,lwd=2)
box()

#Histograma dos dados obtidos pela função densidade.
hist(dados,freq = FALSE,main="",xlab="x",ylab="Density-uni",lwd=2)
lines(x,pfun1(x,a,b),lty=1,lwd=2)
box()
```

```{r chute}

#Log da função de verossimilhança
logdgomp=function(x,dados){
  a=x[1]
  b=x[2]
  -sum(log(b)+a*dados-b/a*(exp(a*dados)-1))
}


achute=1 #Chute inicial para o parâmetro a
bchute=1 #Chute inicial para o parâmetro b
chute=c(achute,bchute)
```
\noindent
A Tabela \ref{tab:a2b3} mostra os resultados para $a=2$ e para $b=3$ das estimativas por Monte Carlo e corrigidas via *Bootstrap*. É visível que em ambos os casos, o viés do corrigido é consideralvemente menor que o do Monte Carlo. No caso do $a,$ todos os erros quadráticos foram menores no caso corrigido, entretanto, no caso do $b,$ todos os erros quadráticos dos estimadores corrigidos foram levemente superiores. Com $n=200,$ a estimativa corrigida de $a$ já acertou até a terceira casa decimal em caso de arredondamento, já a estimativa corrigida do $b$ com $n=200$ acertou até a segunda casa decimal ao arredondar. Nenhuma estimativa de Monte Carlo acertou pontualmente em pelo menos duas casas decimais. A Tabela \ref{tab:ica2b3} mostra que todos os intervalos com 95\% de confiança para cada estimador contiveram o verdadeiro valor do parâmetro.

\begin{table}[H]
\resizebox{\linewidth}{!}{ 
\centering
\begin{tabular}{ll|rrrr|rrrr}
 \hline
& & \multicolumn{4}{c|}{Estimativas para $a=2$} & \multicolumn{4}{c}{Estimativas de $b=3$}\\
  \hline
n & Estimador & Média & Viés & Variância & EQM & Média & Viés & Variância & EQM \\ 
  \hline
    25 & Monte Carlo & 2,5415 & 0,5415 & 1,9746 & 2,2678 & 2,9216 & -0,0784 & 1,0622 & 1,0683 \\ 
    25 & Corrigido & 1,9412 & -0,0588 & 1,9935 & 1,9969 & 3,0113 & 0,0113 & 1,1300 & 1,1301 \\ 
    50 & Monte Carlo & 2,2691 & 0,2691 & 0,8237 & 0,8962 & 2,9540 & -0,0460 & 0,5087 & 0,5108 \\ 
    50 & Corrigido & 1,9791 & -0,0209 & 0,8386 & 0,8391 & 3,0074 & 0,0074 & 0,5389 & 0,5389 \\ 
    100 & Monte Carlo & 2,1491 & 0,1491 & 0,3862 & 0,4084 & 2,9623 & -0,0377 & 0,2505 & 0,2519 \\ 
  100 & Corrigido & 2,0071 & 0,0071 & 0,3913 & 0,3913 & 2,9902 & -0,0098 & 0,2589 & 0,2590 \\ 
  200 & Monte Carlo & 2,0711 & 0,0711 & 0,1794 & 0,1845 & 2,9824 & -0,0176 & 0,1229 & 0,1232 \\ 
   200 & Corrigido & 2,0004 & 0,0004 & 0,1808 & 0,1808 & 2,9969 & -0,0031 & 0,1252 & 0,1252 \\ 
   \hline
\end{tabular}
}
\caption{Comparação dos Estimadores pelos métodos de Monte Carlo e corrigido por Bootstrap para $a=2$ e $b=3$.\label{tab:a2b3}}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ll|rr|rr}
  \hline
& & \multicolumn{2}{c|}{Estimativas para $a=2$} & \multicolumn{2}{c}{Estimativas de $b=3$}\\
  \hline
  n & Estimador & Limite Inferior & Limite Superior & Limite Inferior & Limite Superior \\ 
  \hline
    25 & Monte Carlo &0,2651 & 5,7284 & 1,3015 & 5,2711 \\ 
    25 & Corrigido   &0,0000 & 5,0533 & 1,3261 & 5,4352 \\ 
    50 & Monte Carlo &0,7178 & 4,2411 & 1,7390 & 4,5267 \\ 
    50 & Corrigido   &0,4002 & 3,9604 & 1,7557 & 4,6239 \\ 
   100 & Monte Carlo &1,0380 & 3,4404 & 2,0915 & 4,0425 \\ 
   100 & Corrigido   &0,8706 & 3,3091 & 2,1037 & 4,0872 \\ 
   200 & Monte Carlo &1,2957 & 2,9446 & 2,3342 & 3,7138 \\ 
   200 & Corrigido   &1,2204 & 2,8711 & 2,3464 & 3,7296 \\ 
   \hline
\end{tabular}
\caption{Intervalos com 95$\%$ de confiança do estimadores pelos métodos de Monte Carlo e corrigido por Bootstrap para $a=2$ e $b=3$.\label{tab:ica2b3}}
\end{table}




\noindent
A Tabela \ref{tab:a5b5} mostra os resultados para $a=b=0,5.$ Essa combinação teve resultados similares aos da primeira, ou seja, ambos os parâmetros mantiveram com o viés menor nas estimativas corrigidas, entretanto o EQM da estimativa corrigida do $b$ ficou maior que o de Monte Carlo. A partir de $n=50,$ já é possível perceber um acerto pontual na estimativa em até duas casas decimais, chegando até a 4 com $n=200,$ para $a$ no estimador corrigido, o que não ocorreu em nenhum caso no Monte Carlo. No caso do $b,$ com $n=25$ já era possível ver um acerto com 2 casas decimais no estimador corrigido, entretanto, no Monte Carlo isso só veio a ocorrer com $n=200$. A Tabela \ref{tab:ica5b5} mostra os respectivos limites inferior e superior dos intervalos com 95\% de confiança para $a$ e $b$. Todos contiveram o real valor do parâmetro e no geral tiveram valores bem próximos comparando os corrigidos e não corrigidos em cada tamanho de amostra. 

\begin{table}[H]
\resizebox{\linewidth}{!}{ 
\centering
\begin{tabular}{ll|rrrr|rrrr}
 \hline
& & \multicolumn{4}{c|}{Estimativas para $a=0,5$} & \multicolumn{4}{c}{Estimativas de $b=0,5$}\\
  \hline
n & Estimador & Média & Viés & Variância & EQM & Média & Viés & Variância & EQM \\ 
    25 & Monte Carlo & 0,6011 & 0,1011 & 0,0766 & 0,0869 & 0,4887 & -0,0113 & 0,0322 & 0,0323 \\ 
    25 & Corrigido & 0,4872 & -0,0128 & 0,0774 & 0,0775 & 0,5036 & 0,0036 & 0,0349 & 0,0349 \\ 
   50 & Monte Carlo & 0,5504 & 0,0504 & 0,0326 & 0,0351 & 0,4932 & -0,0068 & 0,0154 & 0,0154 \\ 
    50 & Corrigido & 0,4961 & -0,0039 & 0,0329 & 0,0329 & 0,5013 & 0,0013 & 0,0162 & 0,0162 \\ 
   100 & Monte Carlo & 0,5278 & 0,0278 & 0,0154 & 0,0162 & 0,4942 & -0,0058 & 0,0076 & 0,0076 \\ 
   100 & Corrigido & 0,5013 & 0,0013 & 0,0155 & 0,0155 & 0,4984 & -0,0016 & 0,0078 & 0,0078 \\ 
   200 & Monte Carlo & 0,5132 & 0,0132 & 0,0072 & 0,0074 & 0,4973 & -0,0027 & 0,0037 & 0,0037 \\ 
   200 & Corrigido & 0,5000 & 0,0000 & 0,0072 & 0,0072 & 0,4995 & -0,0005 & 0,0038 & 0,0038 \\ 
   \hline
\end{tabular}
}
\caption{Comparação dos Estimadores pelos métodos de Monte Carlo e corrigido por Bootstrap para $a=0,5$ e $b=0,5$.\label{tab:a5b5}}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ll|rr|rr}
  \hline
& & \multicolumn{2}{c|}{Estimativas para $a=0,5$} & \multicolumn{2}{c}{Estimativas de $b=0,5$}\\
  \hline
  n & Estimador & Limite Inferior & Limite Superior & Limite Inferior & Limite Superior \\ 
  \hline
  25 & Monte Carlo & 0,1469 & 1,2189 & 0,2073 & 0,9030 \\ 
  25 & Corrigido    & 0,0191 & 1,0902 & 0,2116 & 0,9364 \\ 
  50 & Monte Carlo & 0,2396 & 0,9393 & 0,2828 & 0,7687 \\ 
  50 & Corrigido    & 0,1793 & 0,8840 & 0,2856 & 0,7815 \\ 
 100 & Monte Carlo & 0,3016 & 0,7847 & 0,3436 & 0,6819 \\ 
 100 & Corrigido    & 0,2710 & 0,7584 & 0,3449 & 0,6895 \\ 
 200 & Monte Carlo & 0,3567 & 0,6868 & 0,3861 & 0,6239 \\ 
 200 & Corrigido    & 0,3431 & 0,6743 & 0,3871 & 0,6269 \\ 
   \hline
\end{tabular}
\caption{Intervalos com 95$\%$ de confiança do estimadores pelos métodos de Monte Carlo e corrigido por Bootstrap para $a=0,5$ e $b=0,5$.\label{tab:ica5b5}}
\end{table}


```{r tabelas}
tabela25=read.csv2("Tabela25.csv")
tabela50=read.csv2("Tabela50.csv")
tabela100=read.csv2("Tabela100.csv")
tabela200=read.csv2("Tabela200.csv")
a2b3=rbind(tabela25[1:4,],tabela50[1:4,],tabela100[1:4,],tabela200[1:4,])
a2b3=a2b3[order(a2b3$`Parâmetro`),]
#a2b3=a2b3[,c(1,5:8)]
a2b3=a2b3[,c(1,9,10)]
a2b3=cbind(a2b3[1:8,],a2b3[9:16,])
#a2b3=a2b3[,-6]
a2b3=a2b3[,-4]
a5b5=rbind(tabela25[-c(1:4),],tabela50[-c(1:4),],
           tabela100[-c(1:4),],tabela200[-c(1:4),])
a5b5=a5b5[order(a5b5$`Parâmetro`),]
#a5b5=a5b5[,c(1,5:8)]
a5b5=a5b5[,c(1,9,10)]
a5b5=cbind(a5b5[1:8,],a5b5[9:16,])
#a5b5=a5b5[,-6]
a5b5=a5b5[,-4]

#xtable(a2b3,digits=4)
#xtable(a5b5,digits=4)
```

  
```{r estimativas}
#Estimador de máxima verossimilhança 
estimativas=optim(par=chute,fn=logdgomp,dados=dados)$par
ahat=estimativas[1] #Estimador pontual para a
bhat=estimativas[2] #Estimador pontual para b
```
```{r IC}
C=n/bhat^2
B=-sum(exp(ahat*dados)*(1-ahat*dados)-1)/ahat^2
A=bhat/ahat^3*(2*(sum(exp(ahat*dados)*(1-ahat*dados))-1) -
           ahat*sum(exp(ahat*dados)*dados*((1-ahat*dados)^2-1))
         )
varahat=C/(A*C-B^2)
varbhat=A/(A*C-B^2)
covabhat=-B/(A*C-B^2)
ICa=ahat+c(-1,1)*1.96*sqrt(varahat)#Intervalo de confiança para a
ICb=bhat+c(-1,1)*1.96*sqrt(varbhat)#Intervalo de confiança para b
```
```{r var}
varahat=C/(A*C-B^2)
varbhat=A/(A*C-B^2)
covabhat=-B/(A*C-B^2)
ICa=ahat+c(-1,1)*1.96*sqrt(varahat)#Intervalo de confiança para a
ICb=bhat+c(-1,1)*1.96*sqrt(varbhat)
```
```{r informacoes}
# usando n = 25, 50, 100 e 200
# R=5000
# B=500
# 2 configuracoes de parametros
```
# Conclusão
Para a distribuição Gompertz, as estimativas de Monte Carlo com correção de Bootstrap têm viés menor, entretanto não necessariamente uma variância e erro quadrático médio menores que as não corrigidas, por conseguinte, não necessariamente têm um intervalo de confiança menor.

# Referências

A. El-Gohary et al., Applied Mathematical Modelling. 37. ed. Saudi Arabia: Elsevier, 2013.

Guimarães, Uma Função Hiperbólica de Distribuição Probabilística de Alta Flexibilidade. 1. ed. Planaltina: Embrapa, 2002.

# Códigos utilizados
